

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Slurm: script examples &mdash; OzSTAR User Guide  documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="OzSTAR User Guide  documentation" href="../index.html"/>
        <link rel="next" title="Torque-Moab vs Slurm" href="torque-vs-slurm.html"/>
        <link rel="prev" title="Slurm: Creating a job" href="oz-slurm-create.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

      
        <!-- a href="../index.html" -->
        <a href="http://136.186.1.220"><!-- target="_blank"-->
      

      
        
        <img src="../_static/OzStar-FA-Mono-KEYLINE.png" class="logo" />
      
      </a>

      

      
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

      
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Welcome to OzSTAR documentation!</a></li>
</ul>
<p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/Accounts.html">Accounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/Access.html">Access to the supercomputer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/Linux.html">Linux tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/manual.html">How to use man pages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/file-transfer.html">File Transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/Swinburne-HPC.html">Swinburne HPC system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/Filesystems.html">File systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/FAQ.html">FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">OzSTAR vs Green II</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../1a-oz_g2/what-s-new.html">What’s new on OzSTAR?</a></li>
</ul>
<p class="caption"><span class="caption-text">Software</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Modules.html">Environment Modules</a></li>
</ul>
<p class="caption"><span class="caption-text">Jobs on OzSTAR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="oz-partition.html">Job Queues</a></li>
<li class="toctree-l1"><a class="reference internal" href="oz-slurm-basics.html">Slurm: basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="oz-slurm-details.html">Slurm: gathering information</a></li>
<li class="toctree-l1"><a class="reference internal" href="oz-slurm-create.html">Slurm: Creating a job</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Slurm: script examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#message-passing-example-mpi">Message passing example (MPI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#shared-memory-example-openmp">Shared memory example (OpenMP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#embarrassingly-parallel-example">Embarrassingly parallel example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#packed-jobs-example">Packed jobs example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#master-slave-program-example">Master/slave program example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hybrid-jobs">Hybrid jobs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpu-jobs">GPU jobs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interactive-jobs">Interactive jobs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="torque-vs-slurm.html">Torque-Moab vs Slurm</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OzSTAR User Guide</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Slurm: script examples</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="slurm-script-examples">
<h1>Slurm: script examples<a class="headerlink" href="#slurm-script-examples" title="Permalink to this headline">¶</a></h1>
<div class="section" id="message-passing-example-mpi">
<h2>Message passing example (MPI)<a class="headerlink" href="#message-passing-example-mpi" title="Permalink to this headline">¶</a></h2>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#
#SBATCH --job-name=test_mpi
#SBATCH --output=res_mpi.txt
#
#SBATCH --ntasks=4
#SBATCH --time=10:00
#SBATCH --mem-per-cpu=100

module load OpenMPI
srun hello.mpi
</pre></div>
</div>
<p>Request four cores on the cluster for 10 minutes, using 100 MB of RAM per core. Assuming <code class="docutils literal notranslate"><span class="pre">hello.mpi</span></code> was compiled with MPI support, <code class="docutils literal notranslate"><span class="pre">srun</span></code> will create four instances of it, on the nodes allocated by Slurm.</p>
<p>You can try the above example by downloading the <a class="reference external" href="http://en.wikipedia.org/wiki/Message_Passing_Interface#Example_program">example hello world program from Wikipedia</a> (for example, you can name it wiki_mpi_example.c), and compiling it with</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>module load openmpi
mpicc wiki_mpi_example.c -o hello.mpi
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">res_mpi.txt</span></code> file should look something like this:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>0: We have 4 processors
0: Hello 1! Processor 1 reporting for duty
0: Hello 2! Processor 2 reporting for duty
0: Hello 3! Processor 3 reporting for duty
</pre></div>
</div>
</div>
<div class="section" id="shared-memory-example-openmp">
<h2>Shared memory example (OpenMP)<a class="headerlink" href="#shared-memory-example-openmp" title="Permalink to this headline">¶</a></h2>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#
#SBATCH --job-name=test_omp
#SBATCH --output=res_omp.txt
#
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=10:00
#SBATCH --mem-per-cpu=100

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
./hello.omp
</pre></div>
</div>
<p>The job will be run in an allocation where four cores have been reserved on the same compute node.</p>
<p>You can try it by using the <a class="reference external" href="http://en.wikipedia.org/wiki/Openmp#C">hello world program from Wikipedia</a> (for example, you can name it  <code class="docutils literal notranslate"><span class="pre">wiki_omp_example.c</span></code>) and compiling it with</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>gcc -fopenmp wiki_omp_example.c -o hello.omp
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">res_omp.txt</span></code> file should contain something like</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>Hello World from thread 0
Hello World from thread 3
Hello World from thread 1
Hello World from thread 2
There are 4 threads
</pre></div>
</div>
</div>
<div class="section" id="embarrassingly-parallel-example">
<h2>Embarrassingly parallel example<a class="headerlink" href="#embarrassingly-parallel-example" title="Permalink to this headline">¶</a></h2>
<p>This setup is useful for problems based on random draws (e.g. Monte-Carlo simulations). In such cases, you can have four programs drawing 1000 random samples and combining their output afterwards (with another program) you get the equivalent of drawing 4000 samples.</p>
<p>Another typical use of this setting is <strong>parameter sweep</strong>. In this case the same computation is carried on several times by a given code, differing only in the initial value of some high-level parameter for each run. An example could be the optimisation of an integer-valued parameter through range scanning:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#
#SBATCH --job-name=test_emb_arr
#SBATCH --output=res_emb_arr.txt
#
#SBATCH --ntasks=1
#SBATCH --time=10:00
#SBATCH --mem-per-cpu=100
#
#SBATCH --array=1-8

srun ./my_program $SLURM_ARRAY_TASK_ID
</pre></div>
</div>
<p>In this configuration, the command <code class="docutils literal notranslate"><span class="pre">my_program</span></code> will be run eight times, creating eight distinct jobs, each time with a different argument passed with the environment variable defined by slurm <strong>SLURM_ARRAY_TASK_ID</strong> ranging from 1 to 8.</p>
<p>The same idea can be used to process several data files. To do so, we must pass a different input files to different instances of the program by setting the value of the <code class="docutils literal notranslate"><span class="pre">$SLURM_*</span></code> environment variable. For instance, assuming there are exactly eight files in <code class="docutils literal notranslate"><span class="pre">/path/to/data</span></code> we can create the following script:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#
#SBATCH --job-name=test_emb_arr
#SBATCH --output=res_emb_arr.txt
#
#SBATCH --ntasks=1
#SBATCH --time=10:00
#SBATCH --mem-per-cpu=100
#
#SBATCH --array=1-8

FILES=(/path/to/data/*)

srun ./my_program ${FILES[$SLURM_ARRAY_TASK_ID]}
</pre></div>
</div>
<p>In this case, eight jobs will be submitted, each with a different filename given as an argument to <code class="docutils literal notranslate"><span class="pre">my_program</span></code> defined
in the array FILES[].</p>
<p>Note that the same recipe can be used with a numerical argument that is not simply an integer sequence, by defining an array ARGS[] containing the desired values:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>ARGS=(0.05 0.25 0.5 1 2 5 100)

srun ./my_program ${ARGS[$SLURM_ARRAY_TASK_ID]}
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">If the running time of your program is small (i.e. ten minutes or less), creating a job array will incur a lot of overhead and you should consider <em>packing</em> your jobs.</p>
</div>
</div>
<div class="section" id="packed-jobs-example">
<h2>Packed jobs example<a class="headerlink" href="#packed-jobs-example" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">srun</span></code> command has the <code class="docutils literal notranslate"><span class="pre">--exclusive</span></code> argument that allows scheduling independent processes inside a Slurm job allocation. As stated in the documentation:</p>
<div class="line-block">
<div class="line"><em>This option can also be used when initiating more than one job step within an</em></div>
<div class="line"><em>existing resource allocation, where you want separate processors to be</em></div>
<div class="line"><em>dedicated to each job step. If sufficient processors are not available to</em></div>
<div class="line"><em>initiate the job step, it will be deferred. This can be thought of as providing</em></div>
<div class="line"><em>a mechanism for resource management to the job within it’s allocation.</em></div>
<div class="line"><br /></div>
</div>
<p>As an example, the following job submission script will ask Slurm for 8 CPUs, then it will run the <code class="docutils literal notranslate"><span class="pre">myprog</span></code> program 1000 times with arguments passed from 1 to 1000. But with the <code class="docutils literal notranslate"><span class="pre">-n1</span> <span class="pre">--exclusive</span></code> option, it will ensure that at any point in time, only 8 instances are effectively running, each being allocated one CPU.</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#! /bin/bash
#
#SBATCH --ntasks=8
for i in {1..1000}
do
   srun -n1 --exclusive ./myprog $i &amp;
done
wait
</pre></div>
</div>
<p>It is possible to replace the <code class="docutils literal notranslate"><span class="pre">for</span></code>-loop with GNU <code class="docutils literal notranslate"><span class="pre">parrallel</span></code> if available:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>parallel -P $SLURM_NTASKS srun  -n1 --exclusive ./myprog ::: {1..1000}
</pre></div>
</div>
<p>Similarly, many files can be processed with one job submission script. The following script will run <code class="docutils literal notranslate"><span class="pre">myprog</span></code> for every file in <code class="docutils literal notranslate"><span class="pre">/path/to/data</span></code>, but maximum 8 at a time, and using one CPU per task.</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#! /bin/bash
#
#SBATCH --ntasks=8
for file in /path/to/data/*
do
   srun -n1 --exclusive ./myprog $file &amp;
done
wait
</pre></div>
</div>
<p>Similarly as with <code class="docutils literal notranslate"><span class="pre">parallel</span></code>, the <code class="docutils literal notranslate"><span class="pre">for</span></code>-loop can be replaced with another command, <code class="docutils literal notranslate"><span class="pre">xargs</span></code>:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>find /path/to/data -print0 | xargs -0 -n1 -P $SLURM_NTASKS srun -n1 --exclusive ./myprog
</pre></div>
</div>
</div>
<div class="section" id="master-slave-program-example">
<h2>Master/slave program example<a class="headerlink" href="#master-slave-program-example" title="Permalink to this headline">¶</a></h2>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#
#SBATCH --job-name=test_ms
#SBATCH --output=res_ms.txt
#
#SBATCH --ntasks=4
#SBATCH --time=10:00
#SBATCH --mem-per-cpu=100

srun --multi-prog multi.conf
</pre></div>
</div>
<p>With file <code class="docutils literal notranslate"><span class="pre">multi.conf</span></code> being, for example, as follows</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>0      echo I am the Master
1-3    echo I am slave %t
</pre></div>
</div>
<p>The above instructs Slurm to create four tasks (or processes), one running <code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">'I</span> <span class="pre">am</span> <span class="pre">the</span> <span class="pre">Master'</span></code>, and the other 3 running <code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">I</span> <span class="pre">am</span> <span class="pre">slave</span> <span class="pre">%t</span></code>. The <code class="docutils literal notranslate"><span class="pre">%t</span></code> placeholder will be replaced with the task id. This is typically used in a <strong>producer/consumer</strong> setup where one program (the master) create computing tasks for the other program (the slaves) to perform.</p>
<p>Upon completion of the above job, file res_ms.txt will contain</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>I am slave 2
I am slave 3
I am slave 1
I am the Master
</pre></div>
</div>
<p>although not necessarily in the same order.</p>
</div>
<div class="section" id="hybrid-jobs">
<h2>Hybrid jobs<a class="headerlink" href="#hybrid-jobs" title="Permalink to this headline">¶</a></h2>
<p>You can mix multi-processing (MPI) and multi-threading (OpenMP) in the same job, simply like this:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#! /bin/bash
#
#SBATCH --ntasks=8
#SBATCH --ncpus-per-task=4
module load OpenMPI
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
srun ./myprog
</pre></div>
</div>
<p>or even a job array of hybrid jobs:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#! /bin/bash
#
#SBATCH --array=1-10
#SBATCH --ntasks=8
#SBATCH --ncpus-per-task=4
module load OpenMPI
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
srun ./myprog $SLURM_ARRAY_TASK_ID
</pre></div>
</div>
</div>
<div class="section" id="gpu-jobs">
<h2>GPU jobs<a class="headerlink" href="#gpu-jobs" title="Permalink to this headline">¶</a></h2>
<p>Some clusters have a GPU. To request one or more GPUs, there is need to set <code class="docutils literal notranslate"><span class="pre">env</span></code> directories.</p>
<p>To see the if the cluster have a GPU check the generic resources of the computes nodes.</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span># sinfo  -o &quot;%P %.10G %N&quot;
PARTITION       GRES NODELIST
skylake      gpu:1 lmPp[001-003]
</pre></div>
</div>
<p>The slurm command shows 3 nodes with GPU in the post processing partition.</p>
<p>If you want to claim a GPU for your job, you need to specify the GRES (<a class="reference external" href="http://slurm.schedmd.com/gres.html">Generic Resource Scheduling</a>) parameter in your job script. Please note that GPUs are only available in a specific partition whose name depends on the cluster.</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#SBATCH --partition=skylake
#SBATCH --gres=gpu:1
</pre></div>
</div>
<p>A sample job file requesting a node with a GPU could look like this:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --job-name=example
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --ntasks-per-node=1
#SBATCH --time=1:00:00
#SBATCH --mem-per-cpu=1000
#SBATCH --partition=skylake
#SBATCH --gres=gpu:1

module load application/version

myprog input.fits
</pre></div>
</div>
</div>
<div class="section" id="interactive-jobs">
<span id="id1"></span><h2>Interactive jobs<a class="headerlink" href="#interactive-jobs" title="Permalink to this headline">¶</a></h2>
<p>Slurm jobs are normally batch jobs in the sense that they are run unattended. If you want to have a direct view on your job, for tests or debugging, you have two options.</p>
<p>If you need simply to have an interactive Bash session on a compute node, with the same environment set as the batch jobs, run the following command:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span>srun --pty bash
</pre></div>
</div>
<p>Doing that, you are submitting a 1-CPU, default memory, default duration job that will return a Bash prompt when it starts.</p>
<p>If you need more flexibility, you will need to use the <a class="reference external" href="https://slurm.schedmd.com/salloc.html">salloc</a> command. The <code class="docutils literal notranslate"><span class="pre">salloc</span></code> command accepts the same parameters as <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> as far as resource requirement are concerned. Once the allocation is granted, you can use <code class="docutils literal notranslate"><span class="pre">srun</span></code> the same way you would in a submission script.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can access <code class="docutils literal notranslate"><span class="pre">farnarkle1</span></code> and <code class="docutils literal notranslate"><span class="pre">farnarkle2</span></code> as login and interactive nodes.</p>
</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="torque-vs-slurm.html" class="btn btn-neutral float-right" title="Torque-Moab vs Slurm" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="oz-slurm-create.html" class="btn btn-neutral" title="Slurm: Creating a job" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
    
      <style>
        /* Sidebar header (and topbar for mobile) */
        .wy-side-nav-search, .wy-nav-top {
          background: #000000;
        }
        /* Sidebar */
        /*.wy-nav-side {*/
          /*background: #ff0000;*/
        /*}*/
      </style>
    

</body>
</html>