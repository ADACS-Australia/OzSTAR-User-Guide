

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>File systems &mdash; OzSTAR User Guide  documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="OzSTAR User Guide  documentation" href="../index.html"/>
        <link rel="next" title="Environment Modules" href="Modules.html"/>
        <link rel="prev" title="What’s new on OzSTAR?" href="what-s-new.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> OzSTAR User Guide
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/Accounts.html">Accounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/Access.html">Access the supercomputer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/file-transfer.html">File Transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/Profiling.html">Profiling your code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/Swinburne-HPC.html">Swinburne HPC system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1-getting_started/FAQ.html">FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">OzSTAR vs Green II</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="what-s-new.html">What&#8217;s new on OzSTAR?</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">File systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#directories">Directories</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lustre">Lustre</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dealing-with-lustre">Dealing with Lustre</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#striping">1. Striping</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quota">2. Quota</a></li>
<li class="toctree-l3"><a class="reference internal" href="#free-space">3. Free space</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimising-i-o">4. Optimising I/O</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Modules.html">Environment Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="torque-vs-slurm.html">Torque-Moab vs Slurm</a></li>
</ul>
<p class="caption"><span class="caption-text">Jobs on OzSTAR</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/oz-partition.html">Job Queues on OzSTAR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/oz-slurm-basics.html">Slurm: basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/oz-slurm-details.html">Slurm: gathering information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/oz-slurm-create.html">Slurm: Creating a job</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/oz-slurm-examples.html">Slurm: script examples</a></li>
</ul>
<p class="caption"><span class="caption-text">Jobs on Green II</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../3-greenII/g2-Queue.html">Job Queues on Green II</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3-greenII/g2-qsub.html">Submitting Jobs on Green II (qsub)</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OzSTAR User Guide</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>File systems</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="file-systems">
<h1>File systems<a class="headerlink" href="#file-systems" title="Permalink to this headline">¶</a></h1>
<div class="section" id="directories">
<h2>Directories<a class="headerlink" href="#directories" title="Permalink to this headline">¶</a></h2>
<p><strong>OzSTAR</strong></p>
<p>On OzSTAR, The two main directories are:</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>/home

/fred/projects
</pre></div>
</div>
<p>noting that <code class="docutils literal"><span class="pre">/projects</span></code> is soft-linked to <code class="docutils literal"><span class="pre">/fred/projects</span></code>.</p>
<p>As opposed to Green II, both <code class="docutils literal"><span class="pre">/home</span></code> and <code class="docutils literal"><span class="pre">/projects</span></code> directores are using the Lustre filesystem. The <code class="docutils literal"><span class="pre">/home</span></code> directory is backed up. It has a 10GB quota, and 100,000 files. The <code class="docutils literal"><span class="pre">/projects</span></code> directories are on a lustre filesystem. This is where all of your project data should be stored. There is <strong>NO backup for</strong> <code class="docutils literal"><span class="pre">/projects</span></code> <strong>and the user must take responsibility for backing up any data that is important.</strong></p>
<hr class="docutils" />
<p><strong>Green II</strong></p>
<p>On Green II, the two main directories are:</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>/home

/lustre/projects
</pre></div>
</div>
<p>noting that <code class="docutils literal"><span class="pre">/projects</span></code> is soft-linked to <code class="docutils literal"><span class="pre">/lustre/projects</span></code>.</p>
<p>The <code class="docutils literal"><span class="pre">/home</span></code> directory is on NFS and is backed up. It has a 10GB quota. The <code class="docutils literal"><span class="pre">/projects</span></code> directories are on a lustre filesystem. This is where all of your project data should be stored. There is <strong>NO backup for</strong> <code class="docutils literal"><span class="pre">/projects</span></code> <strong>and the user must take responsibility for backing up any data that is important.</strong></p>
</div>
<div class="section" id="lustre">
<h2>Lustre<a class="headerlink" href="#lustre" title="Permalink to this headline">¶</a></h2>
<p><strong>OzSTAR</strong></p>
<p>On OzSTAR, we have ~ 5 Petabytes of Lustre-zfs Filesystem for the OzSTAR nodes. The achievable bandwidth into each OSS (Object Storage Server) is approximately 100 Gb/s. <em>More information to come</em>.</p>
<hr class="docutils" />
<p><strong>Green II</strong></p>
<p>On Green II, we have ~ 3 Petabytes of Lustre Filesystem for the gstar and sstar nodes. At the moment, we have 1 MDS (MetaData Server) to record the metadata of your files and 12 OSSs that serve out data. Each OSS has 8 to 11 OSTs (Object Storage Target). Each OST is a RAID6 or DDP-11 or DDP-20 array of about 14 to 28 TB in capacity.</p>
<p>The achievable bandwidth into each OSS is approximately 1.1 GB/s. The performance of each OST is approximately 350 to 500 MB/s for a single stream of i/o.</p>
<p>In total we have 1180 hard drives and together they are configured as RAID 6 on each of the OSTs. This drive count will increase as part of the upgrade.</p>
<p>Lustre manuals are at <a class="reference external" href="https://wiki.hpdd.intel.com/display/PUB/Documentation">Intel HPDD Wiki</a></p>
</div>
<div class="section" id="dealing-with-lustre">
<h2>Dealing with Lustre<a class="headerlink" href="#dealing-with-lustre" title="Permalink to this headline">¶</a></h2>
<div class="section" id="striping">
<h3>1. Striping<a class="headerlink" href="#striping" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal"><span class="pre">lfs</span> <span class="pre">getstripe/lfs</span> <span class="pre">setstripe</span></code></p>
<p>The use of Lustre striping is <strong>highly discouraged</strong>. Improvements due to striping are limited to a few special and relatively unusual cases in a busy HPC environment, and is only ever acceptable for very large files. The extensive use of wide-striping is highly detrimental to the performance and usability of the Lustre file system as a whole. Do not use striping unless you are sure that it is the correct thing to do.</p>
<p>For more details, please refer to the <a class="reference external" href="http://www.nersc.gov/users/data-and-file-systems/optimizing-io-performance-for-lustre/">NERSC page on striping</a>. Below are listed the criteria for which the use of striping is recommended on g2.</p>
<p><strong>Use cases for which the use of Lustre striping is acceptable:</strong></p>
<ul class="simple">
<li>Huge files that are &gt;few TB in size should be striped so that they do not unduly fill up single OSTs. Lustre file systems are made up of many separate reliable OSTs. Our OSTs are each ~28TB in size. Choose striping so that there is &lt; say 500 GB per OST. So if it was a 5 TB file, then set a striping of 10. Huge files are unwieldy and not recommended for any file system.</li>
<li>Large (&gt;10 GB) single files that are read and written using the MPI IO library from one large parallel job. The MPI IO library allows multiple readers and writers to access and modify a single shared file at the same time. Striping the file such that the number of stripes are &lt;&lt; the number of nodes used by the parallel job should result in some speedup in i/o accesses. Suggested striping might be 4 for a 10GB file.</li>
</ul>
<p>Striping files that are &lt; few GB in size is strongly discouraged. A single process can easily read and write at 300 MB/s to a single un-striped file on Lustre. So a 1GB un-striped file can be read in approximately 3 seconds. There will be much collateral damage to the usability of the whole file system if small 1 GB files are wide-striped. This includes aggregate performance loss due to seek amplification on OSTs and vast interactive performance loss (ls -l could take days) due to not being able to cache OST inodes on the file system servers.</p>
<p>If your files meet the above criteria, or you have previously cleared your use case for striping with the SUT HPC team, then instructions for setting striping on files or directories are below.</p>
<p>Lustre file systems have the ability to stripe data over multiple OSTs. The stripe count can be set on a file system, directory, or file level. Although individual tests may show that striping of &gt;1 improves the read and write performance of &gt; 1GB files on Lustre, overall the use of many striped files will reduce individual and aggregate performance.</p>
<p>To see the current stripe size:</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>[username@g2 ~]$ lfs getstripe /home/username
/home/username
stripe_count:   1 stripe_size:    1048576 stripe_offset:  -1
/home/username/.bash_profile
lmm_stripe_count:   1
lmm_stripe_size:    1048576
lmm_stripe_offset:  72
    obdidx   objid    objid   group
         7  267113  0x41369       0
</pre></div>
</div>
<p>The file <code class="docutils literal"><span class="pre">/home/username/.bash_profile</span></code> is set to stripe in 1 OST and all the writes can start from any OST (stripe offset= -1). The stripe size is set to 1M.</p>
<p>For the command to set stripe, best practise is always leave the stripe offset (-i) as default which is -1. This allows the MDS to choose the starting index. This setting is strongly recommended, as it allows space and load balancing to be done by the MDS as needed. Otherwise, the file starts on the specified OST index.</p>
<p>If you pass the index to 0 and stripe count value of 1, all the files will be written to OST0 until the space is exhausted. This is probably not what you want to do.</p>
<p>If you only want to adjust the stripe count (how many nodes the files write to) and keep the other parameter at their default settings, do not specify any of the other parameters.</p>
<p>To set the stripe count:</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>[username@g2 ~]$ lfs setstripe -c 3 cuda
</pre></div>
</div>
<p>Directory cuda has been set to stripe across 3 OSTs, you can see the output by using the “lfs getstripe” command.</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>[username@g2 ~]$ lfs getstripe cuda
cuda
stripe_count: 3 stripe_size: 0 stripe_offset: -1
</pre></div>
</div>
<p>So when I create a new file inside the cuda directory, it will stripe to 3 OSTs,</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>[username@g2 cuda]$ lfs getstripe test_of_3_osts
test_of_3_osts
lmm_stripe_count:   3
lmm_stripe_size:    1048576
lmm_stripe_offset:  107
    obdidx           objid          objid            group
       107          626536        0x98f68                0
         4          644037        0x9d3c5                0
        14          644985        0x9d779                0
</pre></div>
</div>
<p>The command to set the start index and stripe size:</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>[username@g2 ~]$ lfs setstripe -s 4M -i 1 -c 2 cuda
[username@g2 ~]$ lfs getstripe cuda
cuda
stripe_count:   2 stripe_size:    4194304 stripe_offset:  1
</pre></div>
</div>
<p>The first command shows that the cuda directory has been set to stripe to 2 OSTs, starts from the OST01 and the stripe size is 4M.</p>
<p>Thus, when I create a file in the cuda directory, the new file gets the folder’s stripe size.</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>[username@g2 cuda]$ touch test
[username@g2 cuda]$ lfs getstripe test
test
lmm_stripe_count:   2
lmm_stripe_size:    4194304
lmm_stripe_offset:  1
    obdidx           objid          objid            group
         1          644587        0x9d5eb                0
         2          644065        0x9d3e1                0
</pre></div>
</div>
<p>Further command explanation:</p>
<p><code class="docutils literal"><span class="pre">lfs</span> <span class="pre">setstripe</span></code></p>
<table class="docutils option-list" frame="void" rules="none">
<col class="option" />
<col class="description" />
<tbody valign="top">
<tr><td class="option-group">
<kbd><span class="option">-s</span></kbd></td>
<td>Stripe size (k,m,g). Good stripe is between 1 MB and 4 MB</td></tr>
<tr><td class="option-group">
<kbd><span class="option">-i</span></kbd></td>
<td>OST index of first stripe (default value is -1) Default is recommended.</td></tr>
<tr><td class="option-group">
<kbd><span class="option">-c</span></kbd></td>
<td>Stripe count, number of OST(s) to stripe over (0 default, -1 all)</td></tr>
</tbody>
</table>
</div>
<div class="section" id="quota">
<h3>2. Quota<a class="headerlink" href="#quota" title="Permalink to this headline">¶</a></h3>
<p>Quotas have been enabled on both OzSTAR and Green II.</p>
<p>On <strong>OzSTAR</strong>, the default quota is 10Tb for projects, and 100,000*10 files for projects. If you need extra storage, please contact <a class="reference external" href="mailto:hpc-support&#37;&#52;&#48;swin&#46;edu&#46;au">hpc-support<span>&#64;</span>swin<span>&#46;</span>edu<span>&#46;</span>au</a></p>
<p>On <strong>Green II</strong>, the default quota is 500GB for projects. If you need extra storage, please contact <a class="reference external" href="mailto:hpc-support&#37;&#52;&#48;swin&#46;edu&#46;au">hpc-support<span>&#64;</span>swin<span>&#46;</span>edu<span>&#46;</span>au</a></p>
<p>To check your quota:</p>
<p><code class="docutils literal"><span class="pre">lfs</span> <span class="pre">quota</span> <span class="pre">/lustre</span></code></p>
<p>Output:</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>[username@pbs ~]$ lfs quota  /lustre
Disk quotas for user username (uid 1000):
     Filesystem  kbytes   quota   limit   grace   files   quota   limit   grace
        /lustre [1041408]  5242880 6291456       -     [0]       0       0       -
</pre></div>
</div>
<p>There is also a helpful command for checking the usage of all projects you are associated with. Just type</p>
<p>user-quota
(this needs to be done on the head node g2).</p>
</div>
<div class="section" id="free-space">
<h3>3. Free space<a class="headerlink" href="#free-space" title="Permalink to this headline">¶</a></h3>
<p>The lfs df command shows available disk space on the mounted Lustre file system and space consumption per OST. If multiple Lustre file systems are mounted, a path may be specified, but is not required.</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>root@pbs [~] -&gt;lfs df -h
UUID                       bytes        Used   Available Use% Mounted on
lustre-MDT0000_UUID         1.0T        7.4G      968.6G   1% /lustre[MDT:0]
lustre-OST0000_UUID        14.5T      295.4G       13.5T   2% /lustre[OST:0]
lustre-OST0001_UUID        14.5T      310.0G       13.5T   2% /lustre[OST:1]
lustre-OST0002_UUID        14.5T      324.4G       13.5T   2% /lustre[OST:2]
lustre-OST0003_UUID        14.5T      278.8G       13.5T   2% /lustre[OST:3]
lustre-OST0004_UUID        14.5T      253.1G       13.6T   2% /lustre[OST:4]
lustre-OST0005_UUID        14.5T      238.5G       13.6T   2% /lustre[OST:5]
lustre-OST0006_UUID        14.5T      281.1G       13.5T   2% /lustre[OST:6]
lustre-OST0007_UUID        14.5T      293.7G       13.5T   2% /lustre[OST:7]
lustre-OST0008_UUID        14.5T      261.1G       13.6T   2% /lustre[OST:8]
lustre-OST0009_UUID        14.5T      330.0G       13.5T   2% /lustre[OST:9]
lustre-OST000a_UUID        14.5T      287.7G       13.5T   2% /lustre[OST:10]
(the actual output is longer).
</pre></div>
</div>
</div>
<div class="section" id="optimising-i-o">
<h3>4. Optimising I/O<a class="headerlink" href="#optimising-i-o" title="Permalink to this headline">¶</a></h3>
<p>The best way to get good performance to any filesystem (not just Lustre) is to do reads and writes in large chunks and to use large files rather than many small files.</p>
<p>What is “large” in each context? For Lustre, reads and writes of &gt;1MB are ideal. 10 MB and above is best. Small (eg. &lt; 100 kB) reads and writes and especially 4k random writes are a horrible thing to do to any file system and should be avoided if at all possible. They cause a lot of seeking and obtain low i/o performance from the underlying disks. Small sequential reads are often optimised by read-ahead in block devices or Lustre or on the OSTs so may perform ok, but they’re still not a great idea.</p>
<p>Optimal file sizes are usually between 10 MB and 100 GB. Using anything smaller than 10 MB files risks having its i/o time dominated by open()/close() operations, of which there are a limited amount available to the entire file system. A pathologically bad file usage pattern would be a code that uses 100,000 files, each of &lt;8k in size. This will perform extremely badly on anything except a local SSD in your laptop. It is not a suitable usage model for a large shared supercomputer file system. Similarly, writing a code that has open()/close() in a tight inner loop will be completely dominated by the metadata operations to the Lustre MDS, will perform terribly, and will also impact the use of the cluster for all users because the MDS is a shared resource and can only do a finite number of operations per second.</p>
<p>The best way to get high I/O performance when reading or writing lots of data (eg. a checkpoint) from a large parallel code is generally to write one large O(GB) file per process, or if the number of processes is very large, then one file per node. This will send I/O to all or many of the OSTs that make up the filesystem and so could run as fast as 12 GB/s.</p>
<p>File lock bouncing is also an issue that can affect POSIX parallel file systems. This typically occurs when multiple nodes are appending to the same shared “log” file. However by its very nature the contents of the file are undefined, so it is really a “junk” file. However Lustre will valiantly attempt to interlace I/O from each appending node at the exact moment it writes, leading to a vast amount of “write lock bouncing” between all the appending nodes. This kills the performance all the processes appending, from the nodes doing the appending, and (worst of all) increases the load on the MDS greatly. Do not append to any shared files from multiple nodes. Do not write to any shared files from multiple nodes unless you are going via a library like MPI IO.</p>
</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Modules.html" class="btn btn-neutral float-right" title="Environment Modules" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="what-s-new.html" class="btn btn-neutral" title="What’s new on OzSTAR?" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>