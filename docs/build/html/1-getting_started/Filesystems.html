

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>File systems and I/O &mdash; OzSTAR User Guide  documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="OzSTAR User Guide  documentation" href="../index.html"/>
        <link rel="next" title="FAQ" href="FAQ.html"/>
        <link rel="prev" title="Swinburne HPC system" href="Swinburne-HPC.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

      
        <!-- a href="../index.html" -->
        <a href="http://136.186.1.220"><!-- target="_blank"-->
      

      
        
        <img src="../_static/OzStar-FA-Mono-KEYLINE.png" class="logo" />
      
      </a>

      

      
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

      
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Welcome to OzSTAR documentation!</a></li>
</ul>
<p class="caption"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Accounts.html">Accounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="Access.html">Access to the supercomputer</a></li>
<li class="toctree-l1"><a class="reference internal" href="Linux.html">Linux tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual.html">How to use man pages</a></li>
<li class="toctree-l1"><a class="reference internal" href="file-transfer.html">File Transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="Swinburne-HPC.html">Swinburne HPC system</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">File systems and I/O</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#directories">Directories</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quota">Quota</a></li>
<li class="toctree-l3"><a class="reference internal" href="#transparent-compression">Transparent Compression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hints-for-optimising-lustre-i-o">Hints for Optimising Lustre I/O</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#what-is-large">What is “large”?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#what-are-iops">What are IOPS?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#other-things-to-avoid">Other things to avoid</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#local-disks">Local disks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">OzSTAR vs Green II</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../1a-oz_g2/what-s-new.html">What&#8217;s new on OzSTAR?</a></li>
</ul>
<p class="caption"><span class="caption-text">Software</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/Modules.html">Environment Modules</a></li>
</ul>
<p class="caption"><span class="caption-text">Jobs on OzSTAR</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/oz-partition.html">Job Queues</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/oz-slurm-basics.html">Slurm: basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/oz-slurm-details.html">Slurm: gathering information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/oz-slurm-create.html">Slurm: Creating a job</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/oz-slurm-examples.html">Slurm: script examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2-ozstar/torque-vs-slurm.html">Torque-Moab vs Slurm</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OzSTAR User Guide</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>File systems and I/O</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="file-systems-and-i-o">
<h1>File systems and I/O<a class="headerlink" href="#file-systems-and-i-o" title="Permalink to this headline">¶</a></h1>
<p>The main OzSTAR filesystem is &gt;5 Petabytes (approx 5 PiB) of diskspace in <code class="docutils literal"><span class="pre">/fred</span></code>. This filesystem has more than 30 GB/s of aggregate bandwidth. It is a Lustre + ZFS filesystem with high redundancy.</p>
<p>There are a few other smaller filesystems in the cluster. Of these, <code class="docutils literal"><span class="pre">/home</span></code> and the local <code class="docutils literal"><span class="pre">$JOBFS</span></code> filesystems on SSDs in compute nodes are also worth of discussion. <code class="docutils literal"><span class="pre">/home</span></code> is a Lustre + ZFS filesystem and <code class="docutils literal"><span class="pre">$JOBFS</span></code> is XFS.</p>
<div class="section" id="directories">
<h2>Directories<a class="headerlink" href="#directories" title="Permalink to this headline">¶</a></h2>
<p>On OzSTAR, the two main cluster-wide directories are:</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>/home/&lt;username&gt;
</pre></div>
</div>
<p>and</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>/fred/&lt;project_id&gt;
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">/fred</span></code> is vastly bigger and faster in (almost) every way than <code class="docutils literal"><span class="pre">/home</span></code>, but it is <strong>NOT</strong> backed up. <code class="docutils literal"><span class="pre">/fred</span></code> is meant for large data files and most or all jobs should use this.</p>
<p>The <code class="docutils literal"><span class="pre">/home</span></code> directory is backed up. <code class="docutils literal"><span class="pre">/home</span></code> is meant for small source files and important scripts and input.</p>
<p>Once again - there is <strong>NO backup for</strong> <code class="docutils literal"><span class="pre">/fred</span></code> <strong>and the user must take responsibility for backing up any data that is important (to somewhere other than OzSTAR).</strong></p>
<p>Typically projects will create directories for each of their members inside <code class="docutils literal"><span class="pre">/fred/&lt;project_id&gt;</span></code> (e.g. <code class="docutils literal"><span class="pre">/fred/&lt;project_id&gt;/&lt;username&gt;</span></code>). If a user is a member of multiple projects then they will have access to multiple areas in <code class="docutils literal"><span class="pre">/fred</span></code>.</p>
<div class="section" id="quota">
<h3>Quota<a class="headerlink" href="#quota" title="Permalink to this headline">¶</a></h3>
<p>Disk quotas are enabled on OzSTAR.</p>
<p><code class="docutils literal"><span class="pre">/home</span></code> has a per-user limit of 10GB blocks and 100,000 files. This will not be raised. These numbers are set so that backups are feasible.</p>
<p><code class="docutils literal"><span class="pre">/fred</span></code> has a per-project limit of 10TB blocks and 1M files. If you need extra storage, please contact <a class="reference external" href="mailto:hpc-support&#37;&#52;&#48;swin&#46;edu&#46;au">hpc-support<span>&#64;</span>swin<span>&#46;</span>edu<span>&#46;</span>au</a></p>
<p>Note that because of filesystem compression (see below) it&#8217;s common to store more data than this, and remain under quota. This is because quotas count only actual blocks used on disk.</p>
<p>To check your quota on any node, type:</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span><span class="s">``quota``</span>
</pre></div>
</div>
</div>
<div class="section" id="transparent-compression">
<h3>Transparent Compression<a class="headerlink" href="#transparent-compression" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal"><span class="pre">/fred</span></code> and <code class="docutils literal"><span class="pre">/home</span></code> filesystems have transparent compression turned on. This means that all files (regardless of type) are internally compressed by the filesystem before being stored on disk, and are automatically uncompressed by the filesystem as they are read. This means that you will <em>NOT</em> save diskspace if you gzip your files on OzSTAR. This is unintuitive to many users. eg.</p>
<p>For a highly compressible file like a typical ASCII input file:</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>% ls -ls input
1 -rw-r--r-- 1 root root 39894 Mar  5 17:36 input
</pre></div>
</div>
<p>the file is ~40KB in size, but it uses less than 1KB on disk (the first column).</p>
<p>For a typical binary output file that is somewhat compressible:</p>
<div class="highlight-rst"><div class="highlight"><pre><span></span>% ls -lsh output
7.8M -rw-rw-r-- 1 root root 33M Feb 20 18:25 output
</pre></div>
</div>
<p>so 7.8MB of space on disk is used to store this 33MB file.</p>
<p>This means that although the nominal capacity of <code class="docutils literal"><span class="pre">/fred</span></code> is ~5 PiB (output from <code class="docutils literal"><span class="pre">df</span></code> is somewhat pessimistic), the amount of data that can be stored is greater than this. How much depends upon compressibility of typical data.</p>
<p>Note: If you are transferring files over the network to other machines then it would still make sense to have your files compressed (gzip, bzip, xz etc.) in order to minimise network bandwidth and diskspace at the other end, but internally to OzSTAR there is no need to compress data. Doing so is fine, but unnecessary - it will just result in the data being slightly larger because it is compressed twice by slightly different algorithms.</p>
</div>
<div class="section" id="hints-for-optimising-lustre-i-o">
<h3>Hints for Optimising Lustre I/O<a class="headerlink" href="#hints-for-optimising-lustre-i-o" title="Permalink to this headline">¶</a></h3>
<p>The best way to get good performance to any filesystem (not just Lustre) is to do reads and writes in large chunks and to use large files rather than many small files. Performing many IOPS should be avoided, which often means the same thing.</p>
<div class="section" id="what-is-large">
<h4>What is “large”?<a class="headerlink" href="#what-is-large" title="Permalink to this headline">¶</a></h4>
<p>For Lustre, reads and writes of &gt;4MB are ideal. 10 MB and above is best. Small (eg. &lt; 100 kB) reads and writes and especially 4k random writes should be avoided. These cause seeking and obtain low i/o performance from the underlying raids and disks. Small sequential reads are often optimised by read-ahead in block devices or Lustre or ZFS so may perform acceptably, but it&#8217;s unlikely.</p>
<p>The best way to get high I/O performance from large parallel codes (eg. a checkpoint) is generally to read or write one large O(GB) file per process, or if the number of processes is very large, then one large file per node. This will send I/O to all or many of the 16 large fast OSTs that make up the <code class="docutils literal"><span class="pre">/fred</span></code> filesystem and so could run at over 30 GB/s (not including speed-ups from transparent filesystem compression).</p>
<p>Each of the 16 individual OSTs (Object Storage Target) in the <code class="docutils literal"><span class="pre">/fred</span></code> (dagg) filesystem is composed of 4 large raidz3&#8217;s in a zpool and capable of several GB/s. Because each OST is large and fast there is no reason to use any sort of Lustre striping. Lustre file striping is therefore strongly discouraged for this machine.</p>
</div>
<div class="section" id="what-are-iops">
<h4>What are IOPS?<a class="headerlink" href="#what-are-iops" title="Permalink to this headline">¶</a></h4>
<p>IOPS are Input/Output Operations per Second. i/o operations are things like open, close, seek, read, write, stat, etc.. IOPS is the rate at which these occur.
Optimal cluster file sizes are usually between 10 MB and 100 GB. Using anything smaller than 10 MB files risks having its i/o time dominated by open()/close() operations (IOPS), of which there are a limited amount available to the entire file system. A pathologically bad file usage pattern would be a code that accesses 100,000 files in a row, each of &lt;8k in size. This will perform extremely badly on anything except a local disk. It is not a suitable usage model for a large shared supercomputer file system. Similarly, writing a code that has open()/close() in a tight inner loop will be dominated by the metadata operations to the Lustre MDSs (MetaData Servers), will perform badly, and will also impact the use of the cluster for all users because the MDS is a shared resource and can only do a finite number of operations per second (approx 100k).</p>
</div>
<div class="section" id="other-things-to-avoid">
<h4>Other things to avoid<a class="headerlink" href="#other-things-to-avoid" title="Permalink to this headline">¶</a></h4>
<p>File lock bouncing is also an issue that can affect POSIX parallel file systems. This typically occurs when multiple nodes are appending to the same shared “log” file. However by its very nature the order of the contents of such a file are undefined - it is really a “junk” file. However Lustre will valiantly attempt to interlace I/O from each appending node at the exact moment it writes, leading to a vast amount of “write lock bouncing” between all the appending nodes. This kills the performance all the processes appending, from the nodes doing the appending, and increases the load on the MDS greatly. Do not append to any shared files from multiple nodes. In general a good rule of thumb is to not write at all to the same file from multiple nodes unless it&#8217;s via a library like MPI IO.</p>
</div>
</div>
</div>
<div class="section" id="local-disks">
<h2>Local disks<a class="headerlink" href="#local-disks" title="Permalink to this headline">¶</a></h2>
<p>Each compute node has 350GB of local SSD (fast disk) that is accessible from batch jobs. If the i/o patterns in your workflow are inefficient on the usual cluster filesystems, then you should consider using these local disks.</p>
<p>The <code class="docutils literal"><span class="pre">$JOBFS</span></code> environment variable in each job points at a per-job directory on the local SSD. Space on local disks is requested from <code class="docutils literal"><span class="pre">slurm</span></code> with eg. <code class="docutils literal"><span class="pre">#SBATCH</span> <span class="pre">--tmp=20GB</span></code>. The <code class="docutils literal"><span class="pre">$JOBFS</span></code> directory is automatically created before each job starts, and deleted after the job ends.</p>
<p>A typical workflow that uses local disks would be to copy tar files from <code class="docutils literal"><span class="pre">/fred</span></code> to <code class="docutils literal"><span class="pre">$JOBFS</span></code>, untar, do processing on many small files using many IOPS, tar up the output, copy results back to <code class="docutils literal"><span class="pre">fred</span></code>.</p>
<p>OzSTAR also has 8 large NVME drives that are bigger and faster than these SSDs. Please contact <a class="reference external" href="mailto:hpc-support&#37;&#52;&#48;swin&#46;edu&#46;au">hpc-support<span>&#64;</span>swin<span>&#46;</span>edu<span>&#46;</span>au</a> for information on how to access these.</p>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="FAQ.html" class="btn btn-neutral float-right" title="FAQ" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Swinburne-HPC.html" class="btn btn-neutral" title="Swinburne HPC system" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
    
      <style>
        /* Sidebar header (and topbar for mobile) */
        .wy-side-nav-search, .wy-nav-top {
          background: #000000;
        }
        /* Sidebar */
        /*.wy-nav-side {*/
          /*background: #ff0000;*/
        /*}*/
      </style>
    

</body>
</html>